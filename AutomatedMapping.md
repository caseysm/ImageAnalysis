# Automated Mapping Implementation Plan

## Background

The current imaging pipeline involves manual selection of fiducial points to map between 10X and 40X magnifications. This process is:
- Time-consuming
- Subject to human error
- A bottleneck in high-throughput analysis

We propose to automate this process by leveraging nucleus segmentation data already generated by Cellpose in the pipeline. By matching nuclear positions between magnifications, we can calculate an optimal coordinate transformation automatically.

## Technical Approach

### 1. Overall Workflow

The automated approach will replace the manual mapping procedure with the following steps:

1. **Extract Nucleus Centroids** - Load segmented nuclei from Cellpose outputs
2. **Convert to Global Coordinates** - Transform from tile-specific to well-wide coordinates
3. **Perform Iterative Matching** - Find corresponding nuclei between magnifications
4. **Calculate Optimal Transform** - Determine the 5 DOF affine transformation parameters
5. **Validate & Save Transformation** - Verify accuracy and save for downstream use

### 2. Mathematical Foundation

We will use a 5-degree-of-freedom transformation model:

```
X = scale_x * (cos(θ) * x - sin(θ) * y + dx)
Y = scale_y * (sin(θ) * x + cos(θ) * y + dy)
```

Parameters to optimize:
- Translation: `dx`, `dy`
- Rotation: `θ` (angle)
- Anisotropic scaling: `scale_x`, `scale_y` (typically close to 4.0)

The L-BFGS-B optimization algorithm will be used to find the optimal parameters by minimizing the sum of squared distances between matched nuclei.

## Implementation Requirements

### 1. Core Mapping Module

Create a new module `core/mapping/` with these key components:

- **pipeline.py**: Main mapping pipeline class
- **model.py**: Transformation model and optimization functions
- **matching.py**: Functions for nuclei matching and RANSAC filtering

### 2. Nucleus Data Extraction

Implement functions to:
- Load segmented nuclei masks from both 10X and 40X image results
- Extract centroids for each nucleus
- Convert coordinates to global well-wide coordinate system
- Handle potential variations in nuclei detection between magnifications

### 3. Matching Algorithm

Implement an iterative matching approach:
1. **Initial Transform Guess**:
   - Start with `scale_x = scale_y = 4.0`
   - Use well centers to estimate initial translations
   - Assume zero rotation

2. **Iterative Refinement**:
   - Apply current transform to 10X nuclei
   - Match with nearest 40X nuclei using KD-tree
   - Filter outliers using distance thresholds
   - Apply RANSAC to remove incorrectly matched nuclei
   - Re-optimize transformation parameters
   - Repeat for 5 iterations or until convergence

3. **RANSAC Filtering**:
   - Use random subsets of matched nuclei to estimate transformations
   - Identify and retain inliers that align well with the estimated transform
   - Remove outlier matches to improve robustness

### 4. Optimization Strategies

We will implement two optimization algorithms to allow users to choose the most appropriate method for their data:

#### A. L-BFGS-B Optimization

The Limited-memory Broyden-Fletcher-Goldfarb-Shanno with Bounds (L-BFGS-B) algorithm:
- Approximates the inverse Hessian matrix using gradient information
- Memory-efficient for large numbers of nuclei
- Naturally handles parameter bounds (e.g., scales between 3.5-4.5)
- Implementation:
  ```python
  def optimize_lbfgs(points_10x, points_40x, initial_params):
      def objective(params):
          transformed = model_TRS(points_10x, params)
          return np.sum((transformed - points_40x)**2)
      
      bounds = [(None, None), (None, None), (-10, 10), (3.5, 4.5), (3.5, 4.5)]
      result = minimize(objective, initial_params, method='L-BFGS-B', bounds=bounds)
      return result.x
  ```

#### B. Levenberg-Marquardt Optimization

The Levenberg-Marquardt (LM) algorithm:
- Specialized for least squares problems
- Often achieves greater precision
- Faster convergence with good initial guess
- Implementation:
  ```python
  def optimize_lm(points_10x, points_40x, initial_params):
      def residuals(params):
          transformed = model_TRS(points_10x, params)
          return (transformed - points_40x).flatten()
      
      result = least_squares(residuals, initial_params, method='lm')
      return result.x
  ```

#### Comparative Advantages

| Aspect | L-BFGS-B | Levenberg-Marquardt |
|--------|----------|---------------------|
| Memory usage | Lower (O(m×p) where m=history size) | Higher (O(2n×p) for Jacobian) |
| Large datasets | Excellent | Can be memory-intensive |
| Precision | Good | Excellent near solution |
| Constraint handling | Native bounds support | Limited |
| Speed for small datasets | Good | Very good |
| Convergence guarantee | Less sensitive to initial guess | More sensitive to initial guess |

Both methods will be implemented, with L-BFGS-B as the default for robustness and a command-line flag to use LM for cases where higher precision is needed.

### 5. Validation & Quality Control

Implement validation methods:
- Calculate error metrics (mean, median, max distance between matched nuclei)
- Generate visualization plots:
  - Histogram of matching errors
  - Overlay of matched nuclei
  - Spatial distribution of errors
  - Error vector field visualization
- Provide warnings when errors exceed thresholds

### 6. Integration Points

#### Command-line Interface

Add to `cli.py`:
```python
def run_mapping(args, config, logger):
    """Run the mapping pipeline with the specified parameters."""
    pipeline = MappingPipeline(
        input_file=args.input_file,
        seg_10x_dir=args.seg_10x_dir,
        seg_40x_dir=args.seg_40x_dir,
        output_dir=args.output_dir,
        config_file=args.config,
        optimizer=args.optimizer,  # Allow choosing the optimization method
        **config.get('mapping', {})
    )
    pipeline.run(wells=args.wells)
```

Add optimizer option to the argument parser:
```python
parser.add_argument(
    '--optimizer',
    choices=['lbfgs', 'levenberg-marquardt'],
    default='lbfgs',
    help='Optimization algorithm to use for coordinate mapping (default: lbfgs)'
)
```

#### Standalone Script

Create `bin/run_mapping.py` for direct execution:
```python
#!/usr/bin/env python3
"""Script for automated coordinate mapping between magnifications."""

import argparse
import json
import os
import sys
from pathlib import Path

sys.path.append(str(Path(__file__).parent.parent))

from core.mapping.pipeline import MappingPipeline
from utils.logging import setup_logger

def parse_args():
    """Parse command-line arguments."""
    parser = argparse.ArgumentParser(
        description="Run coordinate mapping between 10X and 40X images"
    )
    
    # Required arguments
    parser.add_argument(
        "input_file",
        help="Path to input ND2 file"
    )
    parser.add_argument(
        "--seg-10x-dir",
        required=True,
        help="Directory containing 10X segmentation results"
    )
    parser.add_argument(
        "--seg-40x-dir",
        required=True,
        help="Directory containing 40X segmentation results"
    )
    
    # Optional arguments
    parser.add_argument(
        "--wells",
        nargs="+",
        help="List of wells to process (default: all wells)"
    )
    parser.add_argument(
        "--output-dir",
        help="Output directory (default: ./results/mapping)"
    )
    parser.add_argument(
        "--optimizer",
        choices=['lbfgs', 'levenberg-marquardt'],
        default='lbfgs',
        help="Optimization algorithm to use (default: lbfgs)"
    )
    parser.add_argument(
        "--config",
        help="Path to JSON configuration file"
    )
    
    return parser.parse_args()

def main():
    """Main entry point."""
    args = parse_args()
    # ... rest of the implementation
```

#### Configuration Settings

Add to `config/settings.py`:
```python
# Mapping settings
MAPPING = {
    # Matching parameters
    'max_iterations': 5,
    'distance_threshold': 50.0,
    'ransac_iterations': 100,
    
    # Optimization parameters
    'default_optimizer': 'lbfgs',  # 'lbfgs' or 'levenberg-marquardt'
    'angle_bounds': (-10.0, 10.0),
    'scale_x_bounds': (3.5, 4.5),
    'scale_y_bounds': (3.5, 4.5),
    
    # L-BFGS-B specific settings
    'lbfgs_ftol': 1e-8,
    'lbfgs_max_iter': 100,
    
    # Levenberg-Marquardt specific settings
    'lm_ftol': 1e-8,
    'lm_xtol': 1e-8,
    'lm_max_nfev': 100,
    
    # ...additional settings
}
```

## Implementation Plan

### Phase 1: Core Development

1. Create the mapping pipeline class structure
2. Implement nuclei loading and coordinate conversion
3. Develop the basic matching algorithm
4. Implement transformation model and both optimization methods (L-BFGS-B and Levenberg-Marquardt)
5. Add selection mechanism to choose between optimization algorithms

### Phase 2: Robustness & Quality Control

1. Add RANSAC filtering for outlier removal
2. Implement validation and error metric calculations
3. Develop visualization tools for quality assessment
4. Add error handling and edge cases

### Phase 3: Integration & Testing

1. Connect with CLI and configuration systems
2. Create standalone script for mapping
3. Test on varied sample data
4. Validate against manually-created mappings

## Expected Challenges

1. **Variation in Nuclei Detection**: Different detection quality between magnifications
   - Solution: Implement adaptive thresholds and robust statistical matching

2. **Sparse or Clustered Nuclei**: Some regions may have insufficient nuclei
   - Solution: Sample nuclei across the entire well, prioritize even spatial distribution

3. **Transformation Complexity**: The real transformation might include non-affine components
   - Solution: Evaluate residual errors to determine if a more complex model is needed

4. **Computational Efficiency**: Processing thousands of nuclei could be time-consuming
   - Solution: Implement efficient KD-tree searching and optimize matrix operations

## Mathematical Details of Optimization Methods

### L-BFGS-B Mathematical Foundation

The L-BFGS-B algorithm works by approximating the inverse Hessian matrix iteratively:

1. **Objective function**: 
   $E(\theta) = \sum_{i=1}^{n} [(X'_i - X_i)^2 + (Y'_i - Y_i)^2]$

2. **Gradient calculation**: 
   $\nabla E(\theta) = \left[\frac{\partial E}{\partial dx}, \frac{\partial E}{\partial dy}, \frac{\partial E}{\partial \alpha}, \frac{\partial E}{\partial s_x}, \frac{\partial E}{\partial s_y}\right]$

3. **Parameter update**:
   $\theta_{k+1} = \theta_k - \alpha_k B_k^{-1} \nabla E(\theta_k)$

Where:
- $B_k^{-1}$ is approximated using the history of parameter and gradient changes
- α is determined by a line search procedure
- Bound constraints are enforced by projection methods

### Levenberg-Marquardt Mathematical Foundation

The LM algorithm combines Gauss-Newton and gradient descent approaches:

1. **Residual vector**:
   $r = [X'_1-X_1, Y'_1-Y_1, X'_2-X_2, Y'_2-Y_2, ..., X'_n-X_n, Y'_n-Y_n]$

2. **Jacobian matrix J**: Contains partial derivatives of each residual with respect to each parameter:
   $J_{2i,1} = \frac{\partial r_{2i}}{\partial dx} = s_x$
   $J_{2i,3} = \frac{\partial r_{2i}}{\partial \alpha} = s_x \cdot (-\sin(\alpha) \cdot x_i - \cos(\alpha) \cdot y_i)$
   ... (and so on for all parameters)

3. **Parameter update**:
   $\theta_{k+1} = \theta_k - (J^T J + \lambda I)^{-1} J^T r$

Where:
- λ is a damping parameter that balances between Gauss-Newton and gradient descent
- The damping is adjusted adaptively based on the success of each iteration

## Conclusion

The automated mapping system will significantly improve the pipeline by:
1. Eliminating manual steps and reducing user burden
2. Improving reproducibility and consistency
3. Speeding up the overall analysis workflow
4. Potentially improving mapping accuracy through the use of many more fiducial points
5. Providing flexibility with multiple optimization algorithms to handle different data characteristics

By leveraging existing Cellpose segmentation results and implementing both L-BFGS-B and Levenberg-Marquardt optimization methods, we can achieve robust and efficient coordinate mapping between magnifications with minimal additional computational overhead, making it a valuable enhancement to the image analysis platform.